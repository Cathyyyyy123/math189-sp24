{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d499b8cf",
   "metadata": {},
   "source": [
    "# Week 9 Summary\n",
    "> NAME: $\\color{red}{\\text{    Chang Shu     }}$\n",
    "> \n",
    "> PID: $\\color{red}{\\text{    A16846972     }}$\n",
    ">\n",
    "> \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bce0cf8",
   "metadata": {},
   "source": [
    "I certify that the following write-up is my own work, and have abided by the UCSD Academic Integrity Guidelines.\n",
    "\n",
    "- [x] Yes\n",
    "- [ ] No"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1c41b1",
   "metadata": {},
   "source": [
    "% # %load tex-macros\n",
    "<div hidden>\n",
    "\\newcommand{\\require}[1]{}\n",
    "\n",
    "$\\require{begingroup}\\require{newcommand}$\n",
    "$\\long\\def \\forcecommand #1{\\providecommand{#1}{}\\renewcommand{#1}}$\n",
    "$\\forcecommand{\\defeq}{\\stackrel{\\small\\bullet}{=}}$\n",
    "$\\forcecommand{\\ra}{\\rangle}$\n",
    "$\\forcecommand{\\la}{\\langle}$\n",
    "$\\forcecommand{\\pr}{{\\mathbb P}}$\n",
    "$\\forcecommand{\\qr}{{\\mathbb Q}}$\n",
    "$\\forcecommand{\\xv}{{\\boldsymbol{x}}}$\n",
    "$\\forcecommand{\\av}{{\\boldsymbol{a}}}$\n",
    "$\\forcecommand{\\bv}{{\\boldsymbol{b}}}$\n",
    "$\\forcecommand{\\cv}{{\\boldsymbol{c}}}$\n",
    "$\\forcecommand{\\dv}{{\\boldsymbol{d}}}$\n",
    "$\\forcecommand{\\ev}{{\\boldsymbol{e}}}$\n",
    "$\\forcecommand{\\fv}{{\\boldsymbol{f}}}$\n",
    "$\\forcecommand{\\gv}{{\\boldsymbol{g}}}$\n",
    "$\\forcecommand{\\hv}{{\\boldsymbol{h}}}$\n",
    "$\\forcecommand{\\nv}{{\\boldsymbol{n}}}$\n",
    "$\\forcecommand{\\sv}{{\\boldsymbol{s}}}$\n",
    "$\\forcecommand{\\tv}{{\\boldsymbol{t}}}$\n",
    "$\\forcecommand{\\uv}{{\\boldsymbol{u}}}$\n",
    "$\\forcecommand{\\vv}{{\\boldsymbol{v}}}$\n",
    "$\\forcecommand{\\wv}{{\\boldsymbol{w}}}$\n",
    "$\\forcecommand{\\zerov}{{\\mathbf{0}}}$\n",
    "$\\forcecommand{\\onev}{{\\mathbf{0}}}$\n",
    "$\\forcecommand{\\phiv}{{\\boldsymbol{\\phi}}}$\n",
    "$\\forcecommand{\\cc}{{\\check{C}}}$\n",
    "$\\forcecommand{\\xv}{{\\boldsymbol{x}}}$\n",
    "$\\forcecommand{\\Xv}{{\\boldsymbol{X}\\!}}$\n",
    "$\\forcecommand{\\yv}{{\\boldsymbol{y}}}$\n",
    "$\\forcecommand{\\Yv}{{\\boldsymbol{Y}}}$\n",
    "$\\forcecommand{\\zv}{{\\boldsymbol{z}}}$\n",
    "$\\forcecommand{\\Zv}{{\\boldsymbol{Z}}}$\n",
    "$\\forcecommand{\\Iv}{{\\boldsymbol{I}}}$\n",
    "$\\forcecommand{\\Jv}{{\\boldsymbol{J}}}$\n",
    "$\\forcecommand{\\Cv}{{\\boldsymbol{C}}}$\n",
    "$\\forcecommand{\\Ev}{{\\boldsymbol{E}}}$\n",
    "$\\forcecommand{\\Fv}{{\\boldsymbol{F}}}$\n",
    "$\\forcecommand{\\Gv}{{\\boldsymbol{G}}}$\n",
    "$\\forcecommand{\\Hv}{{\\boldsymbol{H}}}$\n",
    "$\\forcecommand{\\alphav}{{\\boldsymbol{\\alpha}}}$\n",
    "$\\forcecommand{\\epsilonv}{{\\boldsymbol{\\epsilon}}}$\n",
    "$\\forcecommand{\\betav}{{\\boldsymbol{\\beta}}}$\n",
    "$\\forcecommand{\\deltav}{{\\boldsymbol{\\delta}}}$\n",
    "$\\forcecommand{\\gammav}{{\\boldsymbol{\\gamma}}}$\n",
    "$\\forcecommand{\\etav}{{\\boldsymbol{\\eta}}}$\n",
    "$\\forcecommand{\\piv}{{\\boldsymbol{\\pi}}}$\n",
    "$\\forcecommand{\\thetav}{{\\boldsymbol{\\theta}}}$\n",
    "$\\forcecommand{\\tauv}{{\\boldsymbol{\\tau}}}$\n",
    "$\\forcecommand{\\muv}{{\\boldsymbol{\\mu}}}$\n",
    "$%$\n",
    "$\\forcecommand{\\sd}{\\text{SD}}$\n",
    "$\\forcecommand{\\se}{\\text{SE}}$\n",
    "$\\forcecommand{\\med}{\\text{median}}$\n",
    "$\\forcecommand{\\median}{\\text{median}}$\n",
    "$%$\n",
    "$\\forcecommand{\\supp}{\\text{supp}}$\n",
    "$\\forcecommand{\\E}{\\mathbb{E}}$\n",
    "$\\forcecommand{\\var}{\\text{Var}}$\n",
    "$\\forcecommand{\\Ber}{{\\text{Ber}}}$\n",
    "$\\forcecommand{\\Bin}{{\\text{Bin}}}$\n",
    "$\\forcecommand{\\Geo}{{\\text{Geo}}}$\n",
    "$\\forcecommand{\\Unif}{{\\text{Unif}}}$\n",
    "$\\forcecommand{\\Poi}{{\\text{Poi}}}$\n",
    "$\\forcecommand{\\Exp}{{\\text{Exp}}}$\n",
    "$\\forcecommand{\\Chisq}{{\\chi^2}}$\n",
    "$\\forcecommand{\\N}{\\mathbb{N}}$\n",
    "$\\forcecommand{\\iid}{{\\stackrel{iid}{\\sim}}}$\n",
    "$\\forcecommand{\\px}{p_{X}}$\n",
    "$\\forcecommand{\\fx}{f_{X}}$\n",
    "$\\forcecommand{\\Fx}{F_{X}}$\n",
    "$\\forcecommand{\\py}{p_{Y}}$\n",
    "$\\forcecommand{\\pxy}{p_{X,Y}}$\n",
    "$\\forcecommand{\\po}{{p_0}}$\n",
    "$\\forcecommand{\\pa}{{p_a}}$\n",
    "$\\forcecommand{\\Xbar}{\\overline{X}}$\n",
    "$\\forcecommand{\\Ybar}{\\overline{Y}}$\n",
    "$\\forcecommand{\\Zbar}{\\overline{Z}}$\n",
    "$\\forcecommand{\\nXbar}{n \\cdot \\overline{X}}$\n",
    "$\\forcecommand{\\nYbar}{n \\cdot \\overline{Y}}$\n",
    "$\\forcecommand{\\nZbar}{n \\cdot \\overline{Z}}$\n",
    "$\\forcecommand{\\Xn}{X_1, X_2, \\dots, X_n}$\n",
    "$\\forcecommand{\\Xm}{{X_1, X_2, \\dots, X_m}}$\n",
    "$\\forcecommand{\\Yn}{Y_1, Y_2, \\dots, Y_n}$\n",
    "$\\forcecommand{\\Ym}{{Y_1, Y_2, \\dots, Y_m}}$\n",
    "$\\forcecommand{\\sumXn}{X_1 + X_2 + \\dots + X_n}$\n",
    "$\\forcecommand{\\sumym}{Y_1 + Y_2 + \\dots + Y_m}$\n",
    "$\\forcecommand{\\la}{\\ell_\\alpha}$\n",
    "$\\forcecommand{\\ua}{u_\\alpha}$\n",
    "$\\forcecommand{\\at}{{\\alpha/2}}$\n",
    "$\\forcecommand{\\mux}{\\mu_{X}}$\n",
    "$\\forcecommand{\\muy}{\\mu_{Y}}$\n",
    "$\\forcecommand{\\sx}{\\sigma_{X}}$\n",
    "$\\forcecommand{\\sy}{\\sigma_{Y}}$\n",
    "$\\forcecommand{\\ci}{\\text{CI}}$\n",
    "$\\forcecommand{\\pvalue}{$p$-value}$\n",
    "$\\forcecommand{\\Ho}{H_{0}}$\n",
    "$\\forcecommand{\\Ha}{H_{a}}$\n",
    "\n",
    "\\vskip-\\parskip\n",
    "\\vskip-\\baselineskip\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfec7ec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c6dc53",
   "metadata": {},
   "source": [
    "## Key Takeaways from Week 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc992f31",
   "metadata": {},
   "source": [
    "\n",
    "#### Tuesday: \n",
    "\n",
    "We first went over model selections, which are all possible subsets, stepwise selection, and shrinkage methods. Then, we talked about forward selection, backward selection, and mixed selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d69de7b",
   "metadata": {},
   "source": [
    "#### Thursday\n",
    "\n",
    "We went over regularization such as ridge, lasso, and elastic net.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaadbca8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d78965",
   "metadata": {},
   "source": [
    "## Tue, May 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f374b82",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0650df5",
   "metadata": {},
   "source": [
    "### Variable selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d88520",
   "metadata": {},
   "source": [
    "The reason why we need variable selection is \n",
    "\n",
    "to identify the most important predictors, and include them in the final regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b3024",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccec93a",
   "metadata": {},
   "source": [
    "In class, we discussed three main techniques for variable selection:\n",
    "\n",
    "1. **All possible subsets**: Fit all possible models and select the best one based on some criterion\n",
    "2. **Stepwise selection**: Start with the full or the null model and iteratively add or remove predictors based on some criterion\n",
    "3. **Shrinkage methods**: Use a penalty to shrink the coefficients of the less important predictors to zero\n",
    "\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c1d8f8",
   "metadata": {},
   "source": [
    "##### Selection criterion\n",
    "\n",
    "For the first two methods, we need to define a selection criterion. \n",
    "\n",
    "Some examples of selection criteria include \n",
    "\n",
    "* Full Model\n",
    "* True Model\n",
    "* Adjusted R Squared\n",
    "* Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)\n",
    "* Cross-Validation Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3e42a1",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d06daec",
   "metadata": {},
   "source": [
    "The main drawback with the first method is \n",
    "\n",
    "* As the number of features increases, the number of possible subsets grows exponentially. Computational complexity is too high.\n",
    "* Evaluating all possible subsets can lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fa8a14",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e193521",
   "metadata": {},
   "source": [
    "In the second method, there are three main methods for selecting the best subset of variables. These are \n",
    "\n",
    "1. Forward Selection\n",
    "2. Backward Selection\n",
    "3. Mixed Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf453f0",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266905a9",
   "metadata": {},
   "source": [
    "For an example of the second method, consider the following example \n",
    "\n",
    "Boston housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73b52c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rownames</th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rownames     crim    zn  indus  chas    nox     rm   age     dis  rad  tax  \\\n",
       "0         1  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296   \n",
       "1         2  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242   \n",
       "2         3  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242   \n",
       "3         4  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222   \n",
       "4         5  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222   \n",
       "\n",
       "   ptratio   black  lstat  medv  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://vincentarelbundock.github.io/Rdatasets/csv/MASS/Boston.csv\"\n",
    "boston = pd.read_csv(url)\n",
    "df = boston\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7025c13d",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb91b76",
   "metadata": {},
   "source": [
    "The second method finds a good subset of variables using the following steps:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dbde145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def add(df, selected_columns, columns, criterion, response='medv'):\n",
    "    best_criterion = np.inf\n",
    "    best_column = None\n",
    "    for column in columns - selected_columns:\n",
    "        new_columns = selected_columns.union({column})\n",
    "        formula = f'{response} ~ {\" + \".join(new_columns)}'\n",
    "        current_criterion = criterion(formula, df)\n",
    "        if current_criterion < best_criterion:\n",
    "            best_criterion = current_criterion\n",
    "            best_column = column\n",
    "    return selected_columns.union({best_column}), best_criterion\n",
    "\n",
    "def forward(df, criterion, response='medv'):\n",
    "    selected_columns = set()\n",
    "    columns = set(df.columns.drop(response))\n",
    "    best_criterion = np.inf\n",
    "    while len(selected_columns) < len(columns):\n",
    "        potential_columns, current_criterion = add(df, selected_columns, columns, criterion, response)\n",
    "        if current_criterion > best_criterion:\n",
    "            break\n",
    "        else:\n",
    "            selected_columns = potential_columns\n",
    "            best_criterion = current_criterion\n",
    "            print(f'Criterion: {best_criterion}')\n",
    "    return selected_columns\n",
    "\n",
    "def remove(df, selected_columns, criterion, response='y'):\n",
    "    worst_criterion = np.inf\n",
    "    worst_column = None\n",
    "    for column in selected_columns:\n",
    "        new_columns = selected_columns - {column}\n",
    "        formula = f'{response} ~ {\" + \".join(new_columns)}'\n",
    "        current_criterion = criterion(formula, df)\n",
    "        if current_criterion < worst_criterion:\n",
    "            worst_criterion = current_criterion\n",
    "            worst_column = column\n",
    "    return worst_column, worst_criterion\n",
    "\n",
    "def backward(df, criterion, response='y'):\n",
    "    selected_columns = set(df.columns.drop(response))\n",
    "    columns = set(df.columns.drop(response))\n",
    "    best_criterion = np.inf\n",
    "    while len(selected_columns) > 0:\n",
    "        potential_column, current_criterion = remove(df, selected_columns, criterion, response)\n",
    "        if current_criterion > best_criterion:\n",
    "            break\n",
    "        else:\n",
    "            selected_columns = selected_columns - {potential_column}\n",
    "            best_criterion = current_criterion\n",
    "            print(f'Criterion: {best_criterion}')\n",
    "    return selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17fa9e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criterion: 3295.428030238732\n",
      "Criterion: 3184.2219243070635\n",
      "Criterion: 3131.0034140964826\n",
      "Criterion: 3118.4917282091965\n",
      "Criterion: 3094.7978531829417\n",
      "Criterion: 3087.5248064041275\n",
      "Criterion: 3082.2506760654614\n",
      "Criterion: 3080.3138231220064\n",
      "Criterion: 3083.283323477065\n",
      "Criterion: 3077.165160537028\n",
      "Criterion: 3072.4448278565114\n",
      "vars: {'chas', 'rm', 'ptratio', 'nox', 'tax', 'lstat', 'dis', 'zn', 'rad', 'crim', 'black'}\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "bic_criterion = lambda formula, df: smf.ols(formula, data=df).fit().bic\n",
    "\n",
    "f_vars = forward(df, bic_criterion, response = 'medv')\n",
    "b_vars = backward(df, bic_criterion, response = 'medv')\n",
    "vars = f_vars.union(b_vars)\n",
    "print(f'vars: {vars}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949c9187",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15473cf",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06efcaa3",
   "metadata": {},
   "source": [
    "## Thu, May 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162e5888",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e2f50d",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fdce63",
   "metadata": {},
   "source": [
    "The standard linear model we have looked at so far is\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p + \\epsilon\n",
    "$$\n",
    "\n",
    "which is obtained by solving\n",
    "\n",
    "$$\n",
    "(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2, \\dots, \\hat\\beta_p) = \\mathop{\\arg\\min}\\limits_{\\beta_1 \\dots \\beta_p} L(\\beta_0, \\beta_2, \\dots, \\beta_p)\n",
    "$$\n",
    "\n",
    "\n",
    "The regularization technique inludes a penalty term\n",
    "\n",
    "$$\n",
    "p_\\lambda(\\beta_1, \\dots, \\beta_p)\n",
    "$$\n",
    "\n",
    "And, the new optimization problem is\n",
    "\n",
    "$$\n",
    "L_\\lambda(\\beta_0, \\beta_1, \\dots, \\beta_p) = L(\\beta_0, \\beta_2, \\dots, \\beta_p) + p_\\lambda(\\beta_1, \\dots, \\beta_p)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a298da0",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1849ee39",
   "metadata": {},
   "source": [
    "#### Intuition\n",
    "\n",
    "The intuition of the regularization technique is to\n",
    "\n",
    "use a **penalty** to shrink the coefficients of the less important predictors to zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebe9305",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f002aa",
   "metadata": {},
   "source": [
    "#### Desiderata\n",
    "\n",
    "A good penalty function should satisfy the following properties:\n",
    "\n",
    "1. We want $p_\\lambda(\\beta_1, \\beta_2, \\dots, \\beta_p)$ to be be **large** when too many predictors are included in the model. \n",
    "\n",
    "2. We want $p_\\lambda(\\beta_1, \\beta_2, \\dots, \\beta_p)$ to be **small** when only a few predictors are included in the model.\n",
    "\n",
    "3. Ideally, we want $p_\\lambda(\\beta_1, \\beta_2, \\dots, \\beta_p)$ to be **zero** when only the true predictors are included in the model.\n",
    "\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d7fc59",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71d038b",
   "metadata": {},
   "source": [
    "#### Competing objectives\n",
    "\n",
    "The regularization technique balances two competing objectives:\n",
    "\n",
    "1. the **model fit** since we are minimizing $L(\\beta_0, \\beta_2, \\dots, \\beta_p)$\n",
    "2. the **complexity of the model** since we are also minimizing $p_\\lambda(\\beta_1, \\dots, \\beta_p)$\n",
    "\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1e2315",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f33b7da",
   "metadata": {},
   "source": [
    "#### Penalty functions\n",
    "\n",
    "In class we discussed three different penalty functions:\n",
    "\n",
    "1. The Ridge Penalty\n",
    "2. The Lasso Penalty\n",
    "3. The Elastic Net Penalty\n",
    "\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef3480c",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2143086",
   "metadata": {},
   "source": [
    "#### Penalty function 1\n",
    "\n",
    "The mathematical form is:\n",
    "\n",
    "$$\n",
    "p_\\lambda(\\beta_1, \\dots, \\beta_p) = \\lambda ||{\\betav}||^2 =  \\lambda \\sum_{j=1}^p \\beta_j^2\n",
    "$$\n",
    "\n",
    "Intuitively, this penalty function is trying to favours solutions where all the coefficients are small.\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbd0859",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042ce46c",
   "metadata": {},
   "source": [
    "#### Penalty function 2\n",
    "\n",
    "The mathematical form is:\n",
    "\n",
    "$$\n",
    "p_\\lambda(\\beta_1, \\dots, \\beta_p) = \\lambda ||{\\betav}||_1 =  \\lambda \\sum_{j=1}^p |\\beta_j|\n",
    "$$\n",
    "\n",
    "Intuitively, this penalty function is trying to favours solutions where many of the coefficients are zero.\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87ee28e",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd533aa",
   "metadata": {},
   "source": [
    "#### Penalty function 3\n",
    "\n",
    "The mathematical form is:\n",
    "\n",
    "$$\n",
    "p_\\lambda(\\beta_1, \\dots, \\beta_p) = \\lambda \\Big( \\gamma ||{\\betav}||_1 + (1-\\gamma) ||{\\betav}||^2 \\Big)\n",
    "$$\n",
    "\n",
    "Intuitively, this penalty function is trying to combine of the ridge and lasso penalties.\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41feb894",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d23ab7",
   "metadata": {},
   "source": [
    "#### Penalty function: New\n",
    "\n",
    "Based on my understanding, the following is a fourth candidate penalty function I propose:\n",
    "\n",
    "$$\n",
    "p_\\lambda(\\beta_1, \\dots, \\beta_p) = \\lambda \\Big( \\alpha \\sum_{j=1}^p \\frac{|\\beta_j|^2}{1+|\\beta_j|}  + (1-\\alpha) \\sum_{j=1}^p |{\\betav_j}| \\Big)\n",
    "$$\n",
    "\n",
    "This could be a useful penalty function because it combines the benefits of both $L1$ and $L2$ regularization, while also introducing a novel term that adaptively shrinks larger coefficients.\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea083a4",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2054701",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 2
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
